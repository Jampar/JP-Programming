{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attain features from the input data we must first create a function to extract a column from the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import time as t\n",
    "#27486\n",
    "sample_limit = 100\n",
    "start = t.time()\n",
    "\n",
    "#Import pandas library for data handling.\n",
    "import pandas as pd\n",
    "\n",
    "def get_data_from_column(column_title,file):\n",
    "    #Variable to hold the location of input data.\n",
    "    data_dir = \"/kaggle/input/tweet-sentiment-extraction/\"\n",
    "    #Create the path to the file needed.\n",
    "    file_path = data_dir + file\n",
    "    \n",
    "    #Open the csv file using pandas.\n",
    "    data_file = pd.read_csv(file_path)\n",
    "    #Extract the selected column from the csv file.\n",
    "    data_column = data_file[column_title]\n",
    "    \n",
    "    #Return the column data.\n",
    "    return data_column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract data from any of the csv files, we now need to create a function to produce the N-grams for each of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def generate_ngrams(tweet):\n",
    "    #Generates ngrams in a tuple list\n",
    "    tuple_grams = list(nltk.everygrams(str(tweet).split(' '), 1, len(str(tweet))))\n",
    "    #Converts tuple list to a vertical numpy array\n",
    "    ngrams = np.array([' '.join(i) for i in tuple_grams])[:, None]  \n",
    "    return np.concatenate(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the ngrams can be produced, we can now write the feature extraction functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to encode sentiment infomation to numerical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentiment(sentiment):\n",
    "    if sentiment == \"positive\":\n",
    "        return 1\n",
    "    if sentiment == \"neutral\":\n",
    "        return 0\n",
    "    if sentiment == \"negative\":\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to extract the sentiment scores from a string of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/input/vader-sentiment/\")\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Create an instance of the SentimentIntensityAnalyzer.\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def extract_sentiments_from_text(text):\n",
    "    sentiment_list = []\n",
    "    \n",
    "    #Calculate the sentiment scores for the ngram.\n",
    "    sentiment_scores = analyser.polarity_scores(text)\n",
    "    \n",
    "    #Append each of the sentiment scores to the feature list\n",
    "    for label in list(sentiment_scores):\n",
    "        sentiment_list.append(sentiment_scores[label])\n",
    "        \n",
    "    return sentiment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to extract the POS infomation about an ngram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_list():\n",
    "    tagdict = nltk.load('help/tagsets/upenn_tagset.pickle')\n",
    "    taglist = tagdict.keys()\n",
    "    return list(taglist)\n",
    "\n",
    "tag_list = get_tag_list()\n",
    "\n",
    "def get_POS_info(ngram):\n",
    "    tag_count = [0]* len(tag_list)\n",
    "    \n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(ngram))\n",
    "    \n",
    "    for tag in tagged:\n",
    "        if tag[1] in tag_list:\n",
    "            tag_count[tag_list.index(tag[1])] += 1\n",
    "     \n",
    "    return tag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_caps_count(text):\n",
    "    return sum(map(str.isupper, text.split()))\n",
    "\n",
    "def find_punctuation_count(text):\n",
    "    mark_count = []\n",
    "    mark_lookup = ['!','?','#']\n",
    "    \n",
    "    for mark in mark_lookup:\n",
    "        mark_count.append(text.count(mark))\n",
    "        \n",
    "    return mark_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Implementation for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers as ppb # pytorch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, '/kaggle/input/distilbertbaseuncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function to collate and compile all the features for one ngram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_features(text):\n",
    "    def_features = []\n",
    "    def_features.append(len(text))\n",
    "    def_features += extract_sentiments_from_text(text)\n",
    "    def_features.append(find_all_caps_count(text))\n",
    "    def_features += find_punctuation_count(text)\n",
    "    return def_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_ngram(tweet, tweet_sentiment, ngram):\n",
    "    #List to hold features.\n",
    "    ngram_features = []\n",
    "    \n",
    "    ngram_features += default_features(tweet)\n",
    "    ngram_features.append(encode_sentiment(tweet_sentiment))\n",
    "\n",
    "    ngram_features += default_features(ngram)\n",
    "    ngram_features.append(len(ngram)/len(tweet))    \n",
    "\n",
    "    return ngram_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature order is as follows:\n",
    "\n",
    "* Tweet Length\n",
    "* Tweet Overall Sentiment\n",
    "* Tweet Negative Sentiment Score\n",
    "* Tweet Neutral Sentiment Score\n",
    "* Tweet Positive Sentiment Score\n",
    "* Tweet Compound Sentiment Score\n",
    "* Tweet Punctuation Count\n",
    "* Tweet All Caps Count \n",
    "\n",
    "* N-gram Length\n",
    "* N-gram to Tweet Length Ratio\n",
    "* N-gram Negative Sentiment Score\n",
    "* N-gram Neutral Sentiment Score\n",
    "* N-gram Positive Sentiment Score\n",
    "* N-gram Compound Sentiment Score\n",
    "* Tweet Punctuation Count\n",
    "* Tweet All Caps Count \n",
    "\n",
    "This set of features is extracted for each of the N-grams from one tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_tweet(tweet,tweet_sentiment):\n",
    "    #Array to store all tweet features.\n",
    "    tweet_features = []\n",
    "    \n",
    "    #Generate all N-grams for the tweet.\n",
    "    tweet_ngrams = generate_ngrams(str(tweet))\n",
    "    \n",
    "    #Loop through all the N-grams.\n",
    "    for ngram in tweet_ngrams:\n",
    "        #Extract the features from each N-gram.\n",
    "        ngram_features = extract_features_from_ngram(str(tweet),tweet_sentiment,str(ngram))\n",
    "        #Append the N-gram features to the tweet features.\n",
    "        tweet_features.append(ngram_features)\n",
    "            \n",
    "    return np.array(tweet_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can extract all the features we need from a tweet, we must now calculate the label for each N-gram; the jaccard similarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for jaccard similarity calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2):\n",
    "    a = set(str(str1).lower().split())\n",
    "    b = set(str(str2).lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function to calculate the jaccard similarity for each of the N-grams in a tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_jaccard_labels(tweet, tweet_selected_text):\n",
    "    #List to hold ngram jaccard scores.\n",
    "    labels = []\n",
    "    \n",
    "    #Generate all the ngrams of this tweet.\n",
    "    ngrams = generate_ngrams(tweet)\n",
    "    #Loop through the ngrams generated\n",
    "    for ngram in ngrams:\n",
    "        #Calculate the jaccard score for each ngram.\n",
    "        jaccard_score = jaccard(ngram,tweet_selected_text)\n",
    "        #Append it to the labels list.\n",
    "        labels.append(jaccard_score)\n",
    "    \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can extract both the features and labels from a tweet we can write a function to collate all the features and labels from all of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(tweets,sentiments): \n",
    "    #List to hold all the features of the tweets\n",
    "    features = []\n",
    "    \n",
    "    #Iterate the variable i to the length of the tweets.\n",
    "    for i in range(0,len(tweets)):\n",
    "        #Extract the features from a tweet given its string and sentiment.\n",
    "        tweet_features = extract_features_from_tweet(tweets[i],sentiments[i])\n",
    "        #Append features to total features.\n",
    "        features.append(tweet_features)\n",
    "\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_labels(tweets,selected_texts):\n",
    "    #List to hold labels for all the tweets' ngrams\n",
    "    labels = []\n",
    "    \n",
    "    #Iterate the variable i to the length of the tweets.\n",
    "    for i in range(0,len(tweets)):\n",
    "        #Get the label for all ngrams for a tweet given its string and target string.\n",
    "        tweet_labels = get_ngram_jaccard_labels(tweets[i],selected_texts[i])\n",
    "        #Append labels to total labels.\n",
    "        labels.append(tweet_labels)\n",
    "        \n",
    "    return np.asanyarray(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can bring these two functions together when fitting the regression model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = get_data_from_column(\"text\",\"train.csv\")\n",
    "sentiments = get_data_from_column(\"sentiment\",\"train.csv\")\n",
    "selected_texts = get_data_from_column(\"selected_text\",\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract training features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = np.concatenate(get_all_features(tweets, sentiments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = np.concatenate(get_all_labels(tweets,selected_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LightGBM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f4b941b6080>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "#Compile a training dataset\n",
    "train_data = lgb.Dataset(total_features, label=total_labels)\n",
    "\n",
    "#Set the model's parameters\n",
    "num_round = 10\n",
    "param = {'num_leaves': 500, 'objective': 'huber','boosting': 'dart'}\n",
    "\n",
    "#Train the model\n",
    "bst = lgb.train(param, train_data, num_round)\n",
    "bst.save_model('model.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is fitting with the training data we can apply it to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of all ngrams for all tweets.\n",
    "def get_test_ngrams(tweets):\n",
    "    #List to hold all ngrams.\n",
    "    test_ngrams = []\n",
    "    #Loop through all tweets.\n",
    "    for tweet in tweets:\n",
    "        #Generate the ngrams for that tweet.\n",
    "        ngram = generate_ngrams(tweet)\n",
    "        #Append to that to the total list.\n",
    "        test_ngrams.append([ngram])\n",
    "        \n",
    "    return np.array(test_ngrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = get_data_from_column(\"text\",\"test.csv\")\n",
    "test_sentiments = get_data_from_column(\"sentiment\",\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = get_all_features(test_tweets,test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = lgb.Booster(model_file='model.txt')  # init model\n",
    "\n",
    "\n",
    "#Generate test ngrams.\n",
    "test_ngrams = get_test_ngrams(test_tweets)\n",
    "#List to hold all resulting predicted texts.\n",
    "selected_texts = []\n",
    "\n",
    "#Iterate variable i to the length of the test tweets list\n",
    "for i in range(0,len(test_tweets)):\n",
    "    \n",
    "    #Produce a list of jaccard score predictions for each ngram feature list for each tweet.\n",
    "    prediction = bst.predict(test_features[i], num_iteration=bst.best_iteration)\n",
    "    \n",
    "    #Get the index in that list of the maximum score.\n",
    "    best_prediction_index = list(prediction).index(max(list(prediction)))\n",
    "    \n",
    "    #Get the ngram with the maximum score.\n",
    "    selected_text = test_ngrams[i][0][best_prediction_index]\n",
    "    #Append it to the total list.\n",
    "    selected_texts.append(selected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                selected_text\n",
      "textID                                                       \n",
      "f87dea47db  Last session of the day  http://twitpic.com/67ezh\n",
      "96d74cb729                                    really exciting\n",
      "eee518ae67                                             shame!\n",
      "01082688c6                                              happy\n",
      "33987a8ee5                                          like it!!\n",
      "...                                                       ...\n",
      "e5f0e6ef4b                                              tired\n",
      "416863ce47  Thanks for the net which keeps me alive and ki...\n",
      "6332da480c                                      depression...\n",
      "df1baec676                                               love\n",
      "469e15c5a8                                               cute\n",
      "\n",
      "[3534 rows x 1 columns]\n",
      "1354.8018107414246 Seconds\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "#Converting the selected texts list to numpy array.\n",
    "selected_texts = np.asanyarray(selected_texts)\n",
    "\n",
    "textIDs = get_data_from_column(\"textID\",\"sample_submission.csv\")\n",
    "textIDs = textIDs[:len(selected_texts)]\n",
    "#Create a dataframe from the numpy array using IDs as the index.\n",
    "selected_texts = pd.DataFrame(selected_texts,index=textIDs,columns=[\"selected_text\"])\n",
    "#Output to CSV\n",
    "selected_texts.to_csv('submission.csv')\n",
    "\n",
    "print(selected_texts)\n",
    "end = t.time()\n",
    "print(str(end-start) + \" Seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
